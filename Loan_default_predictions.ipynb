{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Risk scoring and pricing solutions** are commonly used in various industries such as finance, insurance, and healthcare.\n",
        "\n",
        "In finance, risk scoring and pricing solutions are used to assess the creditworthiness of individuals or organizations. Banks and other lending institutions use this information to determine loan terms and interest rates. This helps them to minimize the risk of default and make more informed lending decisions."
      ],
      "metadata": {
        "id": "c6hfKfNzGh8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps Implemented:\n",
        "1. In this example, I first loaded a data set containing information about loan applicants, such as income, age, loan amount, and gender. Then, we preprocessed the data by scaling the numerical features (income, age, loan amount) using MinMaxScaler and encoding the categorical feature (gender) using LabelEncoder.\n",
        "\n",
        "2. Next, I used feature selection by applying chi2 test to select the top 2 features from the data set. We also split the data into training and test sets. \n",
        "\n",
        "3. Then, I trained a logistic regression model and XGBoost model on the training data, and used it to predict the default on the test data and calculated the performance metrics (confusion matrix, precision, recall, and ROC AUC) for the logistic regression model and XGBoost model on the test data set to evaluate the performance of models.\n",
        "\n",
        "4. The compared performance of both the models.\n",
        "\n",
        "Preprocessing and feature selection are important steps in machine learning to improve the performance of the model. By scaling and encoding the features, I ensured that all the features are on the same scale and in the appropriate format for the model to work with. Feature selection is also important to remove redundant or irrelevant features, which can improve the model's performance and reduce over"
      ],
      "metadata": {
        "id": "7mlsb0dLID-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection: There are two ways\n",
        "1. Using web mining methods\n",
        "2. Create synthetic data for model develoment, exploration\n",
        "\n",
        "In this implementation I am focusing on comparision of most widely used models in such applications therefore I will choose to go with 2nd way. "
      ],
      "metadata": {
        "id": "QNvh1NWBJAxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the url you want to scrape\n",
        "url = 'https://example.com/loan-data'\n",
        "\n",
        "# Send a request to the website and parse the HTML content\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Extract the relevant information from the HTML using BeautifulSoup\n",
        "loan_data = []\n",
        "for loan in soup.find_all('div', class_='loan-data'):\n",
        "    income = loan.find('span', class_='income').text\n",
        "    age = loan.find('span', class_='age').text\n",
        "    loan_amount = loan.find('span', class_='loan-amount').text\n",
        "    gender = loan.find('span', class_='gender').text\n",
        "    loan_data.append([income, age, loan_amount, gender])\n",
        "\n",
        "# Convert the extracted data into a Pandas DataFrame\n",
        "data = pd.DataFrame(loan_data, columns=['income', 'age', 'loan_amount', 'gender'])\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('loan_data.csv', index=False)"
      ],
      "metadata": {
        "id": "9Pqm1DgCJmCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ceating synthetic data:\n",
        "\n",
        "I can create a simulated dataset using libraries such as Faker, Numpy, and Pandas. With the help of these libraries, I can generate random and fake data that can be used for testing the model. Here I will use Faker"
      ],
      "metadata": {
        "id": "9lSnzqcPJvfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "afQuDJdVV6NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the Faker library to generate random data for 100,000 loans, including income, age, loan amount, gender, and default. The data is stored in a list called"
      ],
      "metadata": {
        "id": "e9NjRTfVKWkr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM5pqMlR6EJ7"
      },
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Faker object\n",
        "fake = Faker()\n",
        "\n",
        "# Create a list to store the data\n",
        "loan_data = []\n",
        "\n",
        "# Generate data for 100,000 loans\n",
        "for _ in range(100000):\n",
        "    # Generate fake data\n",
        "    income = fake.random_int(min=20000, max=200000, step=100000)\n",
        "    age = fake.random_int(min=18, max=65)\n",
        "    loan_amount = fake.random_int(min=1000, max=20000)\n",
        "    gender = fake.random_element(elements=('male', 'female'))\n",
        "    default = np.random.randint(2)\n",
        "    \n",
        "    # Append data to the list\n",
        "    loan_data.append([income, age, loan_amount, gender, default])\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "data = pd.DataFrame(loan_data, columns=['income', 'age', 'loan_amount', 'gender', 'default'])\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('loan_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of users:\", len(data))"
      ],
      "metadata": {
        "id": "_2dHlOGRFHeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd-xbzOm82Mk"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Explore data insights\n",
        "sns.countplot(x='default', data=data)\n",
        "plt.show()\n",
        "sns.countplot(x='default', hue='gender', data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPe6yz9Q9nnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by age\n",
        "age_groups = data.groupby('age')['income']\n",
        "# Get the statistics of income for each age group\n",
        "income_stats = age_groups.describe()\n",
        "# Print the statistics\n",
        "print(income_stats)"
      ],
      "metadata": {
        "id": "gWSOQgmwWY9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a new column 'age_group' by using the pandas cut function which is used to categorize the data into age groups. Then using the groupby function to group the data by 'age_group' and describe statistical insights like median, max, min, and standard deviation, mean to have a better understanding of the data distribution."
      ],
      "metadata": {
        "id": "q7OqX92hLBR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['age_group']=pd.cut(data['age'], bins=[0, 20, 30, 40, 50, 60, 100], labels=['0-20', '20-30', '30-40', '40-50', '50-60', '60-65'])"
      ],
      "metadata": {
        "id": "6PGM5aR6ajBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('age_group')['income'].describe()"
      ],
      "metadata": {
        "id": "ICMnrYWhcCgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('age_group')['income'].mean()"
      ],
      "metadata": {
        "id": "wjRzTPvo3_K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations:\n",
        "1. "
      ],
      "metadata": {
        "id": "YFi_VIar23KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "#data = pd.read_csv('loan_data.csv')\n",
        "\n",
        "# Preprocessing\n",
        "# Scale numerical features\n",
        "scaler = MinMaxScaler()\n",
        "data[['income', 'age', 'loan_amount']] = scaler.fit_transform(data[['income', 'age', 'loan_amount']])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "IGgb8yQo5rGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "data['gender'] = le.fit_transform(data['gender'])\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "jU3V21SS5yjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "# Select top 2 features using chi2 test\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X = data[['income', 'age', 'loan_amount', 'gender']] # specify features\n",
        "y = data['default'] # specify target variable\n",
        "X_new = selector.fit_transform(X, y)\n",
        "X_new"
      ],
      "metadata": {
        "id": "2kCxk5JU5xQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bQfGtWXRDx3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "2pqxuCOVD09x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Train the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "\n",
        "def eval_performance(y_test, y_pred):\n",
        "  from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "  # Calculate performance metrics\n",
        "  confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "  print(f'Confusion Matrix: {confusion_matrix}')\n",
        "  print(f'Precision: {precision}')\n",
        "  print(f'Recall: {recall}')\n",
        "  print(f'ROC AUC: {roc_auc}')\n",
        "\n",
        "eval_performance(y_test, y_pred)"
      ],
      "metadata": {
        "id": "HclUIH4q3oe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Results:\n",
        "When evaluating a logistic regression model, the confusion matrix, precision, recall, and ROC AUC are commonly used performance metrics.\n",
        "\n",
        "A **confusion matrix** is a table that is used to define the performance of a classification algorithm. The matrix is made up of four values: True Positives (TP), False Positives (FP), True Negatives (TN) and False Negatives (FN).\n",
        "1. True Positives (TP) represent the number of cases in which the model predicted the outcome correctly as positive.\n",
        "2. False Positives (FP) represent the number of cases in which the model predicted the outcome as positive, but it was actually negative.\n",
        "3. True Negatives (TN) represent the number of cases in which the model predicted the outcome correctly as negative.\n",
        "4. False Negatives (FN) represent the number of cases in which the model predicted the outcome as negative, but it was actually positive.\n",
        "In your case, the confusion matrix is [[38 59], [37 66]], this means:\n",
        "\n",
        "38 cases were predicted as true positive (actual positive, predicted positive)\n",
        "59 cases were predicted as false positive (actual negative, predicted positive)\n",
        "37 cases were predicted as false negative (actual positive, predicted negative)\n",
        "66 cases were predicted as true negative (actual negative, predicted negative)\n",
        "\n",
        "**Precision** is the ratio of true positives to the sum of true positives and false positives. Precision is a measure of the accuracy provided that a specific class is predicted. In your case, precision is 0.528, which means that 52.8% of the time the model predicted a positive outcome, it was correct.\n",
        "\n",
        "**Recall** is the ratio of true positives to the sum of true positives and false negatives. Recall is a measure of how many positive cases were correctly identified by the model. In your case, recall is 0.6407766990291263, which means that 64.07766990291263% of the actual positive cases were correctly identified by the model.\n",
        "\n",
        "**ROC AUC** is the receiver operating characteristic (ROC) curve area under the curve (AUC) score. It tells how much model is capable of distinguishing between classes. The score ranges between 0 and 1, where a score of 1 represents a perfect prediction, and a score of 0 represents an incorrect prediction. In your case, ROC AUC is 0.516264638174357, it's value is close to 0.5, this indicates that the model is not able to distinguish between classes correctly.\n",
        "\n",
        "In general, a good logistic regression model should have high precision, recall, and ROC AUC values. However, depending on the specific use case and the desired trade-off between precision and recall, different thresholds may be used to make the final predictions."
      ],
      "metadata": {
        "id": "WXFyAqf5AsZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "s0oRTqQ7CRuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train the model\n",
        "xgboost = XGBClassifier()\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_xg = log_reg.predict(X_test)\n",
        "\n",
        "eval_performance(y_test, y_pred_xg)"
      ],
      "metadata": {
        "id": "27O_XfSxB61z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfuqspuJCY2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the performance of XGBoost and LogisticRegression"
      ],
      "metadata": {
        "id": "yPQwUkkND900"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use cross-validation to compare with XGboost\n",
        "log_reg_cv = cross_val_score(log_reg, X, y, cv=5)\n",
        "xgboost_cv = cross_val_score(xgboost, X, y, cv=5)\n",
        "\n",
        "print(f'Logistic Regression CV Score: {log_reg_cv.mean()}')\n",
        "print(f'XGboost CV Score: {xgboost_cv.mean()}')"
      ],
      "metadata": {
        "id": "8S4UmjgT97RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparision of XGBoost, LogistiRegression\n",
        "\n"
      ],
      "metadata": {
        "id": "kusmphdYHY4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "In this example, we first loaded a data set containing information about loan applicants, such as income, age, loan amount, and gender. Then, we used visualization techniques (countplot) to explore the data insights, and we split the data into training and test sets. Then, we trained a logistic regression model on the training data, and used it to predict the default on the test data.\n",
        "\n",
        "Next, we calculated the performance metrics (confusion matrix, precision, recall, and ROC AUC) for the logistic regression model. Finally, we used cross-validation to compare the performance of the logistic regression model with an XGboost model.\n",
        "\n",
        "The results show the accuracy score"
      ],
      "metadata": {
        "id": "lDLieQavHUz4"
      }
    }
  ]
}