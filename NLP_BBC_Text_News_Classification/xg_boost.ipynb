{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --quiet\n",
    "!pip install keras --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_vals(dframe) -> list:\n",
    "    return sorted([(c, dframe[c].dtype, dframe[c].isnull().sum()) \n",
    "                        for c in dframe.columns if dframe[c].isnull().sum() > 0], \n",
    "                      key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "\n",
    "def metric_report(yTest, yPred):\n",
    "    print(f'accuracy: {accuracy_score(yTest, yPred)}')\n",
    "    print('')\n",
    "    print(\"Classification report: \\n\\n\", metrics.classification_report(yTest, yPred))\n",
    "    print('')\n",
    "    confusion_matrix = metrics.confusion_matrix(yTest, yPred)\n",
    "    print(\"Confusion matrix: \\n\\n\", confusion_matrix)\n",
    "\n",
    "    \n",
    "def clean_txt(txtCol: pd.Series) -> pd.Series:\n",
    "    sw = set(stopwords.words('English'))\n",
    "\n",
    "    text = txtCol.apply(gensim.utils.simple_preprocess, min_len=3)\n",
    "    text = text.apply(lambda s: [w for w in s if w not in sw]) \n",
    "    text = text.apply(lambda s: [SnowballStemmer(\"english\", ignore_stopwords=True).stem(w) for w in s])\n",
    "    text = text.apply(lambda s: ['_'.join(x) for x in nltk.bigrams(s)] + s)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, exclude) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get all txt data, put in list of dicts and return a dataframe\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file not in (exclude):\n",
    "            full_filename = os.path.join(path, file)\n",
    "            for news in os.listdir(full_filename):\n",
    "                with open(os.path.join(full_filename, news), 'rb') as txt_file:\n",
    "                    data.append({'NewsText': txt_file.read(), 'NewsType': file})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def picklefy(txtCol: pd.DataFrame) -> pd.DataFrame:\n",
    "    if [f for f in os.listdir('.') if f.endswith('p')]:\n",
    "        return pickle.load(open('tfidf.p','rb'))\n",
    "    return pickle.dump(clean_txt(txtCol['NewsText']), open('tfidf.p', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsText</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Musicians to tackle US red tape\\n\\nMusicians...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'U2\\'s desire to be number one\\n\\nU2, who hav...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            NewsText       NewsType\n",
       "0  b'Musicians to tackle US red tape\\n\\nMusicians...  entertainment\n",
       "1  b'U2\\'s desire to be number one\\n\\nU2, who hav...  entertainment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preemtively drop duplicates\n",
    "df = get_data('bbc/', 'README.TXT').drop_duplicates(); df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsText</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2127</td>\n",
       "      <td>2127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2127</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>b'Musicians to tackle US red tape\\n\\nMusicians...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 NewsText NewsType\n",
       "count                                                2127     2127\n",
       "unique                                               2127        5\n",
       "top     b'Musicians to tackle US red tape\\n\\nMusicians...    sport\n",
       "freq                                                    1      505"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                        min_df=5, norm='l2', \n",
    "                        encoding='latin-1', \n",
    "                        #ngram_range=(1, 2),\n",
    "                        stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTxtCol = picklefy(df).apply(lambda x: ' '.join(i for i in x))\n",
    "\n",
    "features = tfidf.fit_transform(cleanTxtCol).toarray() \n",
    "labels = df.NewsType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "\n",
    "X = features\n",
    "y = LE.fit_transform(labels)\n",
    "\n",
    "# train/test/split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                            X,y, test_size=0.2\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n"
     ]
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8215962441314554\n",
      "\n",
      "Classification report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77        90\n",
      "           1       0.79      0.74      0.76        72\n",
      "           2       0.83      0.90      0.86        84\n",
      "           3       0.93      0.87      0.90       108\n",
      "           4       0.76      0.79      0.78        72\n",
      "\n",
      "    accuracy                           0.82       426\n",
      "   macro avg       0.82      0.82      0.81       426\n",
      "weighted avg       0.82      0.82      0.82       426\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      " [[70  5  9  1  5]\n",
      " [ 8 53  2  1  8]\n",
      " [ 2  0 76  2  4]\n",
      " [ 6  3  4 94  1]\n",
      " [ 5  6  1  3 57]]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metric_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "xg_clf = XGBClassifier(random_state=42, use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:30:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy: 0.9530516431924883\n",
      "\n",
      "Classification report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        90\n",
      "           1       0.99      0.93      0.96        72\n",
      "           2       0.93      0.95      0.94        84\n",
      "           3       0.98      0.99      0.99       108\n",
      "           4       0.94      0.93      0.94        72\n",
      "\n",
      "    accuracy                           0.95       426\n",
      "   macro avg       0.95      0.95      0.95       426\n",
      "weighted avg       0.95      0.95      0.95       426\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      " [[ 85   0   4   0   1]\n",
      " [  1  67   2   1   1]\n",
      " [  2   0  80   1   1]\n",
      " [  0   0   0 107   1]\n",
      " [  4   1   0   0  67]]\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xg_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "metric_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127 2127\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(len(X),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickcullinane/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:30:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1 7457.620751742\n",
      "2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickcullinane/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:42:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 8087.104672213\n",
      "3\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickcullinane/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:57:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3 9024.106877413\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "clf_xgb = XGBClassifier(objective = 'multi:softmax')\n",
    "param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "              'learning_rate': stats.uniform(0.01, 0.07),\n",
    "              #'subsample': stats.uniform(0.3, 0.7),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              #'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "              #'min_child_weight': [1, 2, 3]\n",
    "             }\n",
    "\n",
    "clf = RandomizedSearchCV(clf_xgb, \n",
    "                         param_distributions = param_dist,\n",
    "                         n_iter = 1, \n",
    "                         scoring = 'f1', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)\n",
    "\n",
    "folds = KFold(n_splits = 3, shuffle = True)\n",
    "\n",
    "n = 1\n",
    "estimators = []\n",
    "results = np.zeros(len(X_train))\n",
    "score = 0.0\n",
    "for train_index, test_index in folds.split(X_train):\n",
    "    print(n)\n",
    "    x_train, X_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    estimators.append(clf.best_estimator_)\n",
    "    results[test_index] = clf.predict(X_test)\n",
    "    score += f1_score(y_test, results[test_index], average='macro')\n",
    "    print(n, time.perf_counter())\n",
    "    n+=1\n",
    "score /= numFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = XGBClassifier(random_state=42, use_label_encoder=False, **clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None,\n",
       "              enable_categorical=False, gamma=None, gpu_id=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=6,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              use_label_encoder=False, validate_parameters=None,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:29:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy: 0.9553990610328639\n",
      "\n",
      "Classification report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       108\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.97      0.92      0.94        91\n",
      "           3       0.97      0.99      0.98        97\n",
      "           4       0.94      0.96      0.95        69\n",
      "\n",
      "    accuracy                           0.96       426\n",
      "   macro avg       0.95      0.95      0.95       426\n",
      "weighted avg       0.96      0.96      0.96       426\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      " [[104   2   1   0   1]\n",
      " [  1  57   2   0   1]\n",
      " [  1   2  84   2   2]\n",
      " [  1   0   0  96   0]\n",
      " [  2   0   0   1  66]]\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xg_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "metric_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 172 has been classified as sport and should be  tech\n",
      "Row 188 has been classified as business and should be  tech\n",
      "Row 253 has been classified as business and should be  tech\n"
     ]
    }
   ],
   "source": [
    "for row_index, (input, prediction, label) in enumerate(zip (X_test, y_pred, y_test)):\n",
    "    if label == 4:\n",
    "        if prediction != label:\n",
    "            print('Row', row_index, 'has been classified as', get_dict_value(prediction), 'and should be ', get_dict_value(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsText    b'Fockers retain film chart crown\\n\\nComedy Me...\n",
      "NewsType                                        entertainment\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[7,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n"
     ]
    }
   ],
   "source": [
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_value(index_value):\n",
    "    return list(le_name_mapping.keys())[list(le_name_mapping.values()).index(index_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
